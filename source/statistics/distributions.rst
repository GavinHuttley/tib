.. jupyter-execute::
    :hide-code:

    import set_working_directory

.. sidebar:: Distribution of plant heights
    :name: plant_heights

    .. jupyter-execute::
        :hide-code:

        import plotly.express as px
        from numpy.random import normal

        x_norm = normal(loc=23, scale=2.0, size=50000)
        fig = px.histogram(x=x_norm, histnorm="probability", height=300, width=400)
        fig.layout.xaxis.title = "Plant Height"
        fig.layout.yaxis.title = "Probability"
        fig.layout.title = "Simulated Plant Heights"
        fig.show()

    Values of "plant height" were generated by using pseudo-random numbers drawn from a Normal distribution function with :math:`\mu=23,\sigma=2`.

.. _pvalues:

******************************
Distributions and the |pvalue|
******************************

In the :ref:`case_studies`, we executed the first 3 of the 4 :ref:`essential steps <essential_steps>` for statistical data analysis. In each case study, the final output was a |chisq| test statistic, degrees-of-freedom and a |pvalue|. In this section we are going to examine the following questions as a precursor to completing the 4th step.

1. What is a |pvalue| and where do they come from?
2. What is a distribution?
3. Where do we get distributions from?

.. All of the above rely on the existence of a model.

.. index:: statistical distribution

In order to answer (1) we really need to answer (2) and (3) first.

.. index::
    statistical distributions

.. _distribution:

What is a distribution?
-----------------------

In statistics, we use the term distribution to describe the occurrence of a random variable. Say we were measuring plant height for a particular species. Plant height will be a "continuously distributed" random variable -- meaning heights can assume an infinite number of values [#]_. If we were to plot those heights along an |xaxis|, then as we continue to obtain new data points we would wind up with having to place points on top of each other. If we continued to do this sampling and plotting we would see a shape emerge (e.g. :ref:`plant heights <plant_heights>`). This shape can be thought of as the distribution of the *plant height random variable*.

.. [#] The values will obviously be bounded, i.e. cannot be negative.

.. index::
    pair: empirical; statistical distributions
    pair: theoretical; statistical distributions
    pair: analytical; statistical distributions

Where do we get distributions from?
-----------------------------------

In the hypothetical example described above, the distribution of :ref:`plant heights <plant_heights>` was obtained from the data itself. This is referred to as an :index:`empirical distribution`.

.. sidebar:: The |chisq| distribution
    :name: independence_test

    .. jupyter-execute::
        :hide-code:

        from numpy import random
        import plotly.express as px

        stats = random.chisquare(9, size=3000)

        fig = px.histogram(x=stats, histnorm="probability",
                           labels=dict(x="ùúí<sup>2</sup>"),
                           height=300, width=400)
        fig.show()

    These values correspond to the distribution of |chisq| values when the null hypothesis is true. (The values were generated by using pseudo-random numbers drawn from a |chisq| distribution function with df=9.)

For those of you who have done introductory courses in statistics you have likely encountered a so-called known :index:`theoretical distributions` [#]_. Examples include the Normal (or Gaussian) distribution, Gamma distribution and the uniform distribution [#]_.

.. [#] These may also be referred to as theoretical or analytical distributions because there are equations that describe them.
.. [#] The uniform distribution is of particular use to the task of understanding |pvalues|

If you choose a measurement that belongs to a known statistical distribution then you get a whole bunch of stuff for free. One thing being the ability to look up the |pvalue| for a test statistic very efficiently.

.. Using a known statistical distribution provides many benefits. you benefit from all the work that has been done on characterising that distribution by others. One is the ability to efficiently determine the |pvalue| of observing your data if the null is true. Other attributes include properties of the statistical test such its sensitivity to small sample sizes, and power of the test. Figuring out whether your particular problem can be addressed using one of the conventional procedures is a great first starting step to efficiently and robustly arriving at a solution.

.. index::
    pair: uniform; statistical distributions
    pair: |Ho|; hypothesis testing
    pair: |pvalue|; hypothesis testing
    pair: test statistic; hypothesis testing

Where do we get |pvalues| from and how do we interpret them?
------------------------------------------------------------

A |pvalue| is a fundamental measure in hypothesis testing that quantifies the consistency of the test statistic with the *null* hypothesis.

In both of our case studies we defined a *reference* condition which we sought to contrast our data with. For example, in :ref:`case study 1 <case_study_1>` the reference condition is one of *independence* between adjacent nucleotides. I use the word reference because both the :index:`test statistic` and its associated |pvalue| are obtained in reference to the distribution of the test statistic *when the null hypothesis is true*. Stated another way, the reference condition corresponds to our *null* hypothesis [#]_.

To really demonstrate what the null, or reference, distribution corresponds to we will now generate one for the :ref:`case study 1 <case_study_1>` problem.

.. [#] In statistics, the words hypothesis and model can be synonyms.

Estimating a |pvalue| computationally
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

With the :download:`case study 1 sample data </data/case_study1.fasta>` we will evaluate whether nucleotides occur randomly along this sequence by writing an algorithm that generates the distribution of our test statistic.

Before we do anything, we need to consider first what data should "look" like if our null hypothesis is correct. This will help us decide how to approach this problem algorithmically.

Our null posits that DNA sequences are just a random ordering of nucleotides. If this were true, we can make a DNA sequence by just sequentially drawing nucleotides randomly from a nucleotide pool until we get the desired sequence length. This process will generate a DNA sequence whose dinucleotide frequencies are consistent with the following probability calculation.

.. math::

    p_{i,j} = p_i\times p_j

Here, given the nucleotide frequencies :math:`p_i` and :math:`p_j`, the expected frequency of the corresponding dinucleotide :math:`i, j` is :math:`p_{i,j}`.

We convert this expected frequency into an expected count (:math:`E_{i,j}`) for a sequence of :math:`\ell` dinucleotides as

.. math::

    E_{i, j} = p_{i, j}\times \ell.

This is a crucial quantity for performing a |chisq| test for independence.

Algorithmic steps for computing a |chisq| statistic
"""""""""""""""""""""""""""""""""""""""""""""""""""

Before proceeding to generate the distribution, let us break the calculations down so that we can write our algorithm. We will use this simple DNA sequence -- ``"AACCCCGT"`` -- to illustrate the steps we need to take in order to be able to compute a chi-square statistic.

#. **Split the sequence into dinucleotides**: From our sample sequence, we need to produce the series of dinucleotides ``["AA", "CC", "CC", "GT"]``.

    .. jupyter-execute::

        def seq_to_dinucs(seq):
            seq = "".join(seq) # for the case when we get seq as a list
            dinucs = [seq[i: i + 2] for i in range(0, len(seq) - 1, 2)]
            return dinucs

        dinucs = seq_to_dinucs("AACCCCGT")
        dinucs

#. **Define a nucleotide order**: We need to make a square matrix of counts where row and columns correspond to specific nucleotides. To enable this we define nucleotides to be in alphabetical order, e.g. ``A`` has index ``0``.

    .. jupyter-execute::

        nucleotide_order = "ACGT"

#. **Convert dinucleotides into pairs of indices**: We use the nucleotide order to convert a dinucleotide string into two integers representing array coordinates, e.g. dinucleotide ``"AA"`` has indices ``(0, 0)`` while ``GT`` has indices ``(2, 3)``. We will do this by writing a function that converts a single dinucleotide into coordinates. Applying this to the sample sequence we get

    .. jupyter-execute::

        def dinuc_to_indices(dinuc):
            return tuple(nucleotide_order.index(nuc) for nuc in dinuc)

        coords = [dinuc_to_indices(dinuc) for dinuc in dinucs]
        coords

#. **Use dinucleotide indices to make a dinucleotide counts matrix**: We use a ``numpy`` array for the counts. Think of the row and column labels for this array as corresponding to the nucleotides present at the first and second position respectively of a dinucleotide. For our example, we get the following

    .. jupyter-execute::

      from numpy import zeros

      def make_counts_matrix(coords):
          counts = zeros((4,4), dtype=int)
          for i, j in coords:
              counts[i, j] += 1
          return counts

      observed = make_counts_matrix(coords)
      observed

#. **Use counts of observed dinucleotides to compute their expected values**: This can be achieved by first generating row and column sums, converting those to frequencies plus a couple of other steps (detail is below).

    .. jupyter-execute::

        from numpy import outer

        def get_expected(counts):
            total = counts.sum() # number of dinucleotides
            row_sums = counts.sum(axis=1)
            col_sums = counts.sum(axis=0)
            # converting to frequencies
            row_probs = row_sums / total
            col_probs = col_sums / total
            # outer product gives us matrix of expected frequencies
            # multiplying by total gives matrix of expected counts
            expecteds = outer(row_probs, col_probs) * total

            return expecteds

        expected = get_expected(observed)
        expected

#. **Generate the** :math:`\mathbf{\chi^2}` **statistic**: This is defined as follows

    .. math::
        :name: eq_chisq

        \chi^2=\sum_i\sum_j\frac{(O_{i,j}-E_{i,j})^2}{E_{i,j}}

    Where :math:`O_{i,j}` and :math:`E_{i,j}` correspond to the observed and expected counts for dinucleotide :math:`i,j`. We express this as a Python function and apply it to our simple example. (The ``numpy`` array operations greatly simplify the calculation.)

    .. jupyter-execute::

        def calc_chisq(observed, expected):
            # observed and expected are both numpy arrays
            chisq = (observed - expected)**2 / expected
            return chisq.sum()

        calc_chisq(observed, expected)

.. note:: The ``nan`` that was output from the ``calc_chisq()`` was generated because we were doing a division with 0 in the denominator. (See output from ``get_expected()`` above.) So time to switch to using the full sequence now.

.. jupyter-execute::

    from cogent3 import load_seq

    seq = load_seq("data/case_study1.fasta", moltype="dna")

Let's provide a simplified interface to all these function calls such that if we provide our sequence, all the above steps are invoked and we get back a |chisq| statistic [#]_.

.. [#] Shuffling generates a permutation of the sequences, which is equivalent to sampling without replacement.

.. jupyter-execute::

    def chiqsq_independent_nucs(seq):
        dinucs = seq_to_dinucs(seq)
        coords = [dinuc_to_indices(dinuc) for dinuc in dinucs]
        observed = make_counts_matrix(coords)
        expected = get_expected(observed)
        return calc_chisq(observed, expected)

    chiqsq_independent_nucs(seq)

So that's nice, we are now able to compute the statistic of interest given a sequence. How do we generate the null?

Generating the null distribution computationally
""""""""""""""""""""""""""""""""""""""""""""""""

To produce a distribution of |chisq| test statistics we need a collection of sequences drawn from the null distribution!

We can generate synthetic sequences consistent with the null by randomly sampling from our actual data. This requires we have a means for producing randomised nucleotide orders from our observed data. Algorithms for generating pseudo-random numbers are important for scientific computing and, as you might expect, there are numerous choices. (Both the Python standard library and ``numpy`` come with a builtin capability for generating such numbers using well regarded algorithms. We will use the one distributed with ``numpy``.)

In our case, we will use a ``shuffle()`` function. Note that ``shuffle()`` works "in place", meaning it modifies the data you provide, so we need to convert our sequence into a list.

.. jupyter-execute::

    from numpy.random import shuffle

    tmp = list("AACCCCGT")
    shuffle(tmp)
    tmp

Will our functions still work if we give them a list?

.. jupyter-execute::

    chiqsq_independent_nucs(list(seq))

Yup!

To recap, the ``chiqsq_independent_nucs()`` function takes a sequence and returns the |chisq| statistic for the independence of the nucleotides at the first and second positions of dinucleotides in that sequence. We want to generate the null distribution for this statistic so that we can assess how unusual the statistic from the case study 1 data is.

We first decide how big a distribution, i.e. how many synthetic sequences, we will generate. Each of these synthetic sequences is generated in accordance with the null by shuffling the original sequence.

The quantities (and corresponding ``variable``) required to estimate the |pvalue| are

:math:`\chi^2_o` (``obs_chisq``)
    The statistic from the original (observed) sequence.

:math:`\chi^2_s` (``syn_chisq``)
    The statistic computed from the synthetic sequences.

:math:`\mathcal{N}` (``num_reps``)
    The number of synthetic sequences to evaluate.

:math:`\mathcal{g}` (``num_gt``)
    The number of synthetic sequences for which :math:`\chi^2_s \ge \chi^2_o`.

From these quantities we estimate the probability of a :math:`\chi^2_o` of equal or greater magnitude occurring by chance under the null model as |pvalue|\ :math:`=\frac{g}{\mathcal{N}}`.

So here's the final function.

.. jupyter-execute::
    
    def estimate_chisq_pval(seq, num_reps):
        obs_chisq = chiqsq_independent_nucs(seq)
        seq = list(seq)
        num_gt = 0
        for i in range(num_reps):
            shuffle(seq)
            syn_chisq = chiqsq_independent_nucs(seq)
            if syn_chisq >= obs_chisq:
                num_gt += 1
        return num_gt / num_reps

    estimate_chisq_pval(seq, 4000)



This type of approach to statistical estimation relies on resampling from observed data is also known as :index:`bootstrapping`.

.. note:: There is a relationship between |pvalues| and :index:`quantiles`. The value returned by ``estimate_chisq_pval()`` is 1 minus the quantile of 22.577 in the simulated :math:`\chi^2_s` distribution.

Estimating a |pvalue| using a theoretical distribution
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The above is my attempt at making concrete the origins of |pvalues|, an essential component to hypothesis testing. I have developed this discourse with a focus on the first of our case studies.

For case study 1, we can also take the more conventional approach of assuming the theoretical |chisq| distribution is appropriate and just use that to "look up" the |pvalue|. Doing so requires we know the degrees-of-freedom (|df|) for our test. In most statistical analysis tools, the df is automatically computed. Certainly for this case, it's not a hard calculation! For the |chisq| applied to a contingency table with :math:`m \times n` rows and columns :math:`df=(m-1) \times (n-1)`, which is 9 in this case. The result is what we saw earlier

.. jupyter-execute::
    :hide-code:

    from cogent3.maths.stats.number import CategoryCounter
    from cogent3.maths.stats.contingency import CategoryCounts

    c = CategoryCounter([(n1, n2) for n1, n2 in seq_to_dinucs(seq)])
    c = CategoryCounts(c)
    c.chisq_test().statistics

As you can see, this |pvalue| is close to that estimated above [#]_. The interpretation is the same, we would expect a :math:`\chi^2_9\ge`\ 22.577 will occur by chance 0.0072 of the time when the null hypothesis is true.

.. [#] For resampling approaches, the estimated |pvalue| will converge on the theoretical value with increasing ``num_reps``. That said, this statement is not universally true -- for a 4bp long sequence there are only 256 possible synthetic sequences.

.. sidebar:: Corollaries from the definition of |pvalues|
    :name: pvalue_corollaries
    
    The definition of |pvalues| has some very import corollaries.
    
    #. You can obtain a very small |pvalue| even if the null hypothesis is true, just by chance.
    #. A small |pvalue| does not mean the null hypothesis is disproven.
    #. A large |pvalue| does not mean the null hypothesis is proven.

.. todo:: add an exercise, rewrite the ``estimate_chisq_pval()`` function so that it computes the quantile of the distribution and uses that to estimate the |pvalue|
